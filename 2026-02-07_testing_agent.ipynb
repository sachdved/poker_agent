{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78120e48-668b-448d-a044-1bdb8d90151e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb  7 21:09:59 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.119.02             Driver Version: 580.119.02     CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5090        Off |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   24C    P8             23W /  450W |     304MiB /  32607MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            1955      G   /usr/bin/kwin_wayland                    13MiB |\n",
      "|    0   N/A  N/A            1962      G   /usr/bin/sddm-greeter-qt6               115MiB |\n",
      "|    0   N/A  N/A            1996      G   /usr/bin/maliit-keyboard                 72MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614e553e-c95e-4605-965a-a8fe261cfdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachdved/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "from ml_modules import *\n",
    "from sequence_modules import *\n",
    "from llm_modules import *\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138735ff-59a4-4356-be2e-370fd9a0caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "street_embedder = StreetPositionalEncoding(\n",
    "    num_streets = 4,\n",
    "    embedding_dim = 256,\n",
    "    max_seq_len = 1024,\n",
    "    device = \"cuda\"\n",
    ")\n",
    "\n",
    "table_position_embedder = TablePositionalEncoding(\n",
    "    num_players = 3,\n",
    "    embedding_dim = 256,\n",
    "    max_seq_len = 1024,\n",
    "    device = \"cuda\"\n",
    ")\n",
    "\n",
    "action_embedder = ActionEncoding(\n",
    "    #num_actions = 21,\n",
    "    embedding_dim = 256,\n",
    "    max_seq_len = 1024,\n",
    "    device = \"cuda\"\n",
    ")\n",
    "\n",
    "pot_size_embedder = PotSizeSequenceEmbedder(\n",
    "    max_seq_len = 1024,\n",
    "    pad_value = -1,\n",
    "    device = 'cuda'\n",
    ")\n",
    "\n",
    "poker_sequence_embedder = PokerSequenceEmbedder(\n",
    "    street_input_dimension = 256,\n",
    "    table_position_input_dimension = 256,\n",
    "    action_input_dimension = 256,\n",
    "    latent_dimensions = [256, 512, 1024, 2048],\n",
    "    device = 'cuda'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ce7c1c-8266-4f93-b6a7-f975217c3919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "street_idxs = torch.Tensor([\n",
    "    [0, 0, 6, 6, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 6, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 0, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 0, 0, 6, 6, 6],\n",
    "    [0, 0, 0, 0, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 0, 0, 4, 6, 6],\n",
    "    [0, 0, 0, 0, 0, 4, 6, 6],\n",
    "    [0, 0, 0, 0, 0, 4, 1, 6],\n",
    "])\n",
    "print(street_idxs.shape)\n",
    "table_position_idxs = torch.Tensor([\n",
    "    [0, 1, 3, 3, 3, 3, 3, 3],\n",
    "    [0, 1, 2, 3, 3, 3, 3, 3],\n",
    "    [0, 1, 2, 0, 3, 3, 3, 3],\n",
    "    [0, 1, 2, 0, 1, 3, 3, 3],\n",
    "    [0, 1, 2, 0, 3, 3, 3, 3],\n",
    "    [0, 1, 2, 0, 1, 3, 3, 3],\n",
    "    [0, 1, 2, 0, 1, 3, 3, 3],\n",
    "    [0, 1, 2, 0, 3, 0, 3, 3],\n",
    "])\n",
    "print(table_position_idxs.shape)\n",
    "action_idxs = torch.Tensor([\n",
    "    [0, 1, 21, 21, 21, 21, 21, 21],\n",
    "    [0, 1, 4, 21, 21, 21, 21, 21],\n",
    "    [0, 1, 2, 2, 21, 21, 21, 21],\n",
    "    [0, 1, 2, 4, 3, 21, 21, 21],\n",
    "    [0, 1, 2, 4, 21, 21, 21, 21],\n",
    "    [0, 1, 2, 4, 3, 19, 21, 21],\n",
    "    [0, 1, 4, 2, 3, 19, 21, 21],\n",
    "    [0, 1, 6, 2, 4, 19, 17, 21]\n",
    "])\n",
    "print(action_idxs.shape)\n",
    "pot_size_sequence = torch.Tensor([\n",
    "    [1, 3, -1, -1, -1, -1, -1, -1],\n",
    "    [1, 3, 5, -1, -1, -1, -1, -1],\n",
    "    [1, 3, 3, 3, -1, -1, -1, -1],\n",
    "    [1, 3, 3, 4, 4, -1, -1, -1],\n",
    "    [1, 3, 3, 4, -1, -1, -1, -1],\n",
    "    [1, 3, 3, 4, 4, -1, -1, -1],\n",
    "    [1, 3, 5, 5, 5, 5, -1, -1],\n",
    "    [1, 3, 7, 7, 9, 9, 9, -1]\n",
    "])\n",
    "print(pot_size_sequence.shape)\n",
    "active_players = torch.Tensor([\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [0, 1, 1]\n",
    "])\n",
    "print(active_players.shape)\n",
    "stack_size = torch.Tensor(\n",
    "    [\n",
    "        [399, 398, 400],\n",
    "        [399, 398, 398],\n",
    "        [399, 398, 400],\n",
    "        [398, 398, 400],\n",
    "        [398, 398, 400],\n",
    "        [398, 398, 400],\n",
    "        [399, 398, 398],\n",
    "        [399, 396, 396]\n",
    "    ]\n",
    ")\n",
    "stack_size.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c12edcd6-063f-4eec-8eb7-524958ca21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_modules import *\n",
    "\n",
    "cards = Cards()\n",
    "\n",
    "deck_order_shuffled = torch.argsort(torch.rand(8, 52))\n",
    "\n",
    "card_embeddings = cards(deck_order_shuffled%13, deck_order_shuffled//13)\n",
    "\n",
    "card_unshown_embedding = cards(\n",
    "    torch.Tensor([[13]]).to(dtype = torch.long), torch.Tensor([[4]]).to(dtype = torch.long)\n",
    ")\n",
    "\n",
    "card_unshown_embedding = card_unshown_embedding.tile([8, 5, 1])\n",
    "\n",
    "cards_player_0_embeddings = torch.concat([card_embeddings[:, :2, :], card_unshown_embedding], dim = 1)\n",
    "cards_player_1_embeddings = torch.concat([card_embeddings[:, 2:4, :], card_unshown_embedding], dim = 1)\n",
    "cards_player_2_embeddings = torch.concat([card_embeddings[:, 4:6, :], card_unshown_embedding], dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b1c4ffd-ff05-4ef3-bb4f-533d98f559f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokerAgent(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Full poker agent. Contains information about the cards,\n",
    "    the players, the board, the sequence embedding, and the \n",
    "    probability prediction model.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        street_embedder : torch.nn.Module,\n",
    "        table_position_embedder : torch.nn.Module,\n",
    "        action_embedder : torch.nn.Module,\n",
    "        pot_size_embedder : torch.nn.Module,\n",
    "        llm : transformers.AutoModelForCausalLM,\n",
    "        policy_model : torch.nn.Module,\n",
    "        device : str = \"cpu\",\n",
    "        llm_train : bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.llm_train = llm_train\n",
    "        self.street_embedder = street_embedder\n",
    "        self.table_position_embedder = table_position_embedder\n",
    "        self.action_embedder = action_embedder\n",
    "        self.pot_size_embedder = pot_size_embedder\n",
    "        self.poker_sequence_embedder = poker_sequence_embedder  # This was missing the definition\n",
    "        self.llm = llm\n",
    "        self.policy_model = policy_model\n",
    "        \n",
    "        if not llm_train:\n",
    "            for parameter in self.llm.parameters():\n",
    "                parameter.requires_grad = False\n",
    "        \n",
    "        # Remove the hook from __init__ - it's defined twice and the first one does nothing\n",
    "       \n",
    "    def forward(\n",
    "        self,\n",
    "        street_idxs : torch.Tensor,\n",
    "        table_position_idxs : torch.Tensor,\n",
    "        action_idxs : torch.Tensor,\n",
    "        pot_size_sequence : torch.Tensor,\n",
    "        active_players : torch.Tensor,\n",
    "        stack_size : torch.Tensor,\n",
    "        card_embeddings : torch.Tensor\n",
    "    ):\n",
    "        street_idxs_out, street_embs = self.street_embedder(street_idxs)\n",
    "        street_embedding = {\n",
    "            'street_idxs': street_idxs_out,\n",
    "            'street_embedding': street_embs,\n",
    "        }\n",
    "        \n",
    "        table_pos_idxs_out, table_pos_embs = self.table_position_embedder(table_position_idxs)\n",
    "        table_position_embedding = {\n",
    "            'table_position_idxs': table_pos_idxs_out,\n",
    "            'table_position_embedding': table_pos_embs,\n",
    "        }\n",
    "        \n",
    "        action_idxs_out, action_embs = self.action_embedder(action_idxs)\n",
    "        action_embedding = {\n",
    "            'action_idxs': action_idxs_out,\n",
    "            'action_embedding': action_embs,\n",
    "        }\n",
    "        padded_pot_size_sequence = self.pot_size_embedder(pot_size_sequence)\n",
    "        \n",
    "        model_inputs = (\n",
    "            street_embedding \n",
    "            | \n",
    "            table_position_embedding \n",
    "            | \n",
    "            action_embedding \n",
    "            | \n",
    "            {'pot_size_sequence' : padded_pot_size_sequence.unsqueeze(2)}\n",
    "            |\n",
    "            {\n",
    "                'active_players' : active_players,\n",
    "                'stack_size' : stack_size,\n",
    "                'card_embeddings' : card_embeddings\n",
    "            }\n",
    "        )\n",
    "        model_inputs['attention_mask'] = (model_inputs['pot_size_sequence'] != -1).squeeze(-1).to('cuda')\n",
    "        \n",
    "        # Use a dict to capture the activation (mutable object)\n",
    "        activation_cache = {'activation': None}\n",
    "        \n",
    "        def hook(module, input, output):\n",
    "            activation_cache['activation'] = output\n",
    "        \n",
    "        handle = self.llm.model.layers[27].post_attention_layernorm.register_forward_hook(hook)\n",
    "        \n",
    "        try:\n",
    "            inputs_embeds = self.poker_sequence_embedder(model_inputs).to(device=\"cuda\", dtype=torch.bfloat16)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.llm(inputs_embeds=inputs_embeds, attention_mask=model_inputs['attention_mask'])\n",
    "            \n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # Get activation from the cache\n",
    "        activation = activation_cache['activation']\n",
    "        \n",
    "        if activation is None:\n",
    "            raise RuntimeError(\"Hook did not capture activation - check layer path\")\n",
    "        \n",
    "        activations_last_action = activation[\n",
    "            torch.arange(activation.shape[0]), \n",
    "            model_inputs['attention_mask'].sum(dim=1) - 1, \n",
    "            :\n",
    "        ]\n",
    "        \n",
    "        model_inputs['llm_state'] = activations_last_action\n",
    "        model_inputs['probits'] = self.policy_model(\n",
    "                model_inputs['active_players'].to(self.device),\n",
    "                model_inputs['stack_size'].to(self.device),\n",
    "                model_inputs['card_embeddings'].to(self.device),\n",
    "                model_inputs['llm_state'].to(self.device)\n",
    "        )\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1af59fb-ce8c-4480-92c7-00376f00cdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.97it/s]\n",
      "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"./models/qwen3-1point7b/\"\n",
    "\n",
    "\n",
    "tokenizer, model = load_model(model_name)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40d2d854-c152-47a4-be9b-58a38fb97601",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = PolicyModel(\n",
    "    num_players = 3,\n",
    "    active_players_hidden_dims = [1024, 2048],\n",
    "    stack_size_hidden_dims = [1024, 2048],\n",
    "    card_embeddings_hidden_dims = [2048, 2048],\n",
    "    final_output_hidden_dims = [1024, 512, 256],\n",
    "    device = 'cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c37c9267-218f-41f7-907e-4adadcb9faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "poker_player = PokerAgent(\n",
    "    street_embedder,\n",
    "    table_position_embedder,\n",
    "    action_embedder,\n",
    "    pot_size_embedder,\n",
    "    model,\n",
    "    policy_model,\n",
    "    device = 'cuda',\n",
    "    llm_train = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78cfa7b-ee4a-4b9c-a90e-fd803c99eea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb  7 21:10:02 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.119.02             Driver Version: 580.119.02     CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5090        Off |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   26C    P1             64W /  450W |    4731MiB /  32607MiB |      8%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            1955      G   /usr/bin/kwin_wayland                    13MiB |\n",
      "|    0   N/A  N/A            1962      G   /usr/bin/sddm-greeter-qt6               115MiB |\n",
      "|    0   N/A  N/A            1996      G   /usr/bin/maliit-keyboard                 72MiB |\n",
      "|    0   N/A  N/A            7390      C   ...local_llm_host/bin/python3.11       4418MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7374069-d85a-4ad4-a2ac-6032297f7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = poker_player(\n",
    "    street_idxs.to('cuda'),\n",
    "    table_position_idxs.to('cuda'),\n",
    "    action_idxs.to('cuda'), \n",
    "    pot_size_sequence.to('cuda'),\n",
    "    active_players.to('cuda'),\n",
    "    stack_size.to('cuda'),\n",
    "    cards_player_0_embeddings.to('cuda')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a6babe-41fb-46d4-80c9-279a2098540c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb  7 21:10:02 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.119.02             Driver Version: 580.119.02     CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5090        Off |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   37C    P1            148W /  450W |    9031MiB /  32607MiB |     99%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            1955      G   /usr/bin/kwin_wayland                    13MiB |\n",
      "|    0   N/A  N/A            1962      G   /usr/bin/sddm-greeter-qt6               115MiB |\n",
      "|    0   N/A  N/A            1996      G   /usr/bin/maliit-keyboard                 72MiB |\n",
      "|    0   N/A  N/A            7390      C   ...local_llm_host/bin/python3.11       8718MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88562c43-428d-4a98-9ee1-12b6970598c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After forward pass\n",
      "  Allocated: 3.71 GB\n",
      "  Reserved:  8.52 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def print_gpu_memory(label=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"{label}\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved:  {reserved:.2f} GB\")\n",
    "        print()\n",
    "\n",
    "print_gpu_memory(\"After forward pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c338668b-7750-4036-bcb2-8a22f3eaaab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After gc.collect()\n",
      "  Allocated: 3.68 GB\n",
      "  Reserved:  8.52 GB\n",
      "\n",
      "After empty_cache()\n",
      "  Allocated: 3.68 GB\n",
      "  Reserved:  4.11 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del outputs\n",
    "\n",
    "gc.collect()\n",
    "print_gpu_memory(\"After gc.collect()\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_memory(\"After empty_cache()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6161fd4a-a01b-4038-880d-b07ebd76d348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5b7c3-1864-448a-b0c7-45b84ef153c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
