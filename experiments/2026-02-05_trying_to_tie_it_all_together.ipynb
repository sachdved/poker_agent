{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bae0722-57f8-426a-961c-7139206f4086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachdved/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "from ml_modules import *\n",
    "from sequence_modules import *\n",
    "from llm_modules import *\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312d3e42-95fd-402d-bc26-2ce0eee4528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "street_embedder = StreetPositionalEncoding(\n",
    "    num_streets = 4,\n",
    "    embedding_dim = 256,\n",
    "    max_seq_len = 1024,\n",
    "    device = \"cpu\"\n",
    ")\n",
    "\n",
    "table_position_embedder = TablePositionalEncoding(\n",
    "    num_players = 2,\n",
    "    embedding_dim = 256,\n",
    "    max_seq_len = 1024,\n",
    "    device = \"cpu\"\n",
    ")\n",
    "\n",
    "action_embedder = ActionEncoding(\n",
    "    #num_actions = 21,\n",
    "    embedding_dim = 256,\n",
    "    max_seq_len = 1024,\n",
    "    device = \"cpu\"\n",
    ")\n",
    "\n",
    "pot_size_embedder = PotSizeSequenceEmbedder(\n",
    "    max_seq_len = 1024,\n",
    "    pad_value = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67ce607-8e01-4e50-8733-95fc37380ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 8])\n",
      "torch.Size([12, 8])\n",
      "torch.Size([11, 8])\n",
      "torch.Size([11, 8])\n",
      "torch.Size([11, 2])\n"
     ]
    }
   ],
   "source": [
    "street_idxs = torch.Tensor([\n",
    "    [0, 0, 6, 6, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 6, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 0, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 0, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 0, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 0, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 0, 4, 6, 6, 6], \n",
    "    [0, 0, 0, 0, 4, 0, 0, 6], \n",
    "    [0, 0, 0, 0, 4, 0, 0, 6], \n",
    "    [0, 0, 0, 0, 4, 0, 0, 6], \n",
    "    [0, 0, 0, 0, 4, 0, 0, 6], \n",
    "])\n",
    "print(street_idxs.shape)\n",
    "table_position_idxs = torch.Tensor([\n",
    "    [0, 1, 2, 2, 2, 2, 2, 2],\n",
    "    [0, 1, 0, 2, 2, 2, 2, 2],\n",
    "    [0, 1, 0, 1, 2, 2, 2, 2],\n",
    "    [0, 1, 0, 1, 2, 2, 2, 2],\n",
    "    [0, 1, 0, 1, 2, 2, 2, 2],\n",
    "    [0, 1, 0, 1, 2, 2, 2, 2],\n",
    "    [0, 1, 0, 1, 2, 2, 2, 2],\n",
    "    [0, 1, 0, 1, 2, 0, 1, 2],\n",
    "    [0, 1, 0, 1, 2, 0, 1, 2],\n",
    "    [0, 1, 0, 1, 2, 0, 1, 2],\n",
    "    [0, 1, 0, 1, 2, 0, 1, 2],\n",
    "    [0, 1, 2, 2, 2, 2, 2, 2],\n",
    "])\n",
    "print(table_position_idxs.shape)\n",
    "action_idxs = torch.Tensor([\n",
    "    [0, 1, 21, 21, 21, 21, 21, 21],\n",
    "    [0, 1, 4, 21, 21, 21, 21, 21],\n",
    "    [0, 1, 5, 4, 21, 21, 21, 21],\n",
    "    [0, 1, 4, 3, 21, 21, 21, 21],\n",
    "    [0, 1, 5, 6, 21, 21, 21, 21],\n",
    "    [0, 1, 5, 2, 21, 21, 21, 21],\n",
    "    [0, 1, 5, 4, 19, 21, 21, 21],\n",
    "    [0, 1, 5, 4, 19, 3, 3, 21],\n",
    "    [0, 1, 5, 4, 19, 5, 2, 21],\n",
    "    [0, 1, 5, 4, 19, 5, 4, 21],\n",
    "    [0, 1, 5, 4, 19, 3, 5, 21],\n",
    "])\n",
    "print(action_idxs.shape)\n",
    "pot_size_sequence = torch.Tensor([\n",
    "    [1, 3, -1, -1, -1, -1, -1, -1],\n",
    "    [1, 3, 5, -1, -1, -1, -1, -1],\n",
    "    [1, 3, 7, 8, -1, -1, -1, -1],\n",
    "    [1, 3, 5, 5, -1, -1, -1, -1],\n",
    "    [1, 3, 7, 15, -1, -1, -1, -1],\n",
    "    [1, 3, 7, 7, -1, -1, -1, -1],\n",
    "    [1, 3, 7, 8, 8, -1, -1, -1],\n",
    "    [1, 3, 7, 8, 8, 8, 8, -1],\n",
    "    [1, 3, 7, 8, 8, 10, 10, -1],\n",
    "    [1, 3, 7, 8, 8, 10, 12, -1],\n",
    "    [1, 3, 7, 8, 8, 8, 10, -1],\n",
    "])\n",
    "print(pot_size_sequence.shape)\n",
    "active_players = torch.Tensor([\n",
    "    [1, 1],\n",
    "    [1, 1],\n",
    "    [1, 1],\n",
    "    [1, 1],\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [1, 1],\n",
    "])\n",
    "print(active_players.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f8a5bd-220b-4bcf-82d6-a28bb26c0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = PokerActionValidator(\n",
    "    num_players = 2,\n",
    "    small_blind = 1,\n",
    "    big_blind = 2,\n",
    "    starting_stack_sizes = 400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9734b45-4c07-4eb5-8435-23d652908cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_actions = test.get_legal_actions_mask(\n",
    "    street_idxs,\n",
    "    table_position_idxs,\n",
    "    action_idxs, \n",
    "    pot_size_sequence,\n",
    "    active_players\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d755f6-b5f5-4bc3-a4e9-eea3a9d04451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fb624f5-e674-496d-b151-7a9be0562de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "        False])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_actions[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d6fb89-e424-4c12-965a-7a1a5e7eb183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1, -2, -2,  0, -1,  0, -2, -1, -2,  0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.get_next_to_act(\n",
    "    street_idxs,\n",
    "    table_position_idxs,\n",
    "    action_idxs, \n",
    "    active_players\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4707efe0-2a49-420d-b7d2-c8a37c883201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 8])\n",
      "torch.Size([9, 8])\n",
      "torch.Size([9, 8])\n",
      "torch.Size([9, 8])\n",
      "torch.Size([9, 3])\n"
     ]
    }
   ],
   "source": [
    "street_idxs = torch.Tensor([\n",
    "    [0, 0, 6, 6, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 6, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 0, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 0, 0, 6, 6, 6],\n",
    "    [0, 0, 0, 0, 6, 6, 6, 6],\n",
    "    [0, 0, 0, 0, 0, 4, 6, 6],\n",
    "    [0, 0, 0, 0, 0, 4, 6, 6],\n",
    "    [0, 0, 0, 0, 0, 4, 1, 6],\n",
    "    [0, 0, 0, 0, 0, 4, 6, 6]\n",
    "])\n",
    "print(street_idxs.shape)\n",
    "table_position_idxs = torch.Tensor([\n",
    "    [0, 1, 3, 3, 3, 3, 3, 3],\n",
    "    [0, 1, 2, 3, 3, 3, 3, 3],\n",
    "    [0, 1, 2, 0, 3, 3, 3, 3],\n",
    "    [0, 1, 2, 0, 1, 3, 3, 3],\n",
    "    [0, 1, 2, 0, 3, 3, 3, 3],\n",
    "    [0, 1, 2, 0, 1, 3, 3, 3],\n",
    "    [0, 1, 2, 0, 1, 3, 3, 3],\n",
    "    [0, 1, 2, 0, 3, 0, 3, 3],\n",
    "    [0, 1, 2, 0, 2, 3, 3, 3],\n",
    "])\n",
    "print(table_position_idxs.shape)\n",
    "action_idxs = torch.Tensor([\n",
    "    [0, 1, 21, 21, 21, 21, 21, 21],\n",
    "    [0, 1, 4, 21, 21, 21, 21, 21],\n",
    "    [0, 1, 2, 2, 21, 21, 21, 21],\n",
    "    [0, 1, 2, 4, 3, 21, 21, 21],\n",
    "    [0, 1, 2, 4, 21, 21, 21, 21],\n",
    "    [0, 1, 2, 4, 3, 19, 21, 21],\n",
    "    [0, 1, 4, 2, 3, 19, 21, 21],\n",
    "    [0, 1, 5, 2, 4, 19, 17, 21],\n",
    "    [0, 1, 5, 2, 4, 19, 21, 21],\n",
    "])\n",
    "print(action_idxs.shape)\n",
    "pot_size_sequence = torch.Tensor([\n",
    "    [1, 3, -1, -1, -1, -1, -1, -1],\n",
    "    [1, 3, 5, -1, -1, -1, -1, -1],\n",
    "    [1, 3, 3, 3, -1, -1, -1, -1],\n",
    "    [1, 3, 3, 4, 4, -1, -1, -1],\n",
    "    [1, 3, 3, 4, -1, -1, -1, -1],\n",
    "    [1, 3, 3, 4, 4, -1, -1, -1],\n",
    "    [1, 3, 5, 5, 5, 5, -1, -1],\n",
    "    [1, 3, 7, 7, 9, 9, 9, -1],\n",
    "    [1, 3, 7, 7, 9, 9, -1, -1]\n",
    "])\n",
    "print(pot_size_sequence.shape)\n",
    "active_players = torch.Tensor([\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [0, 1, 1],\n",
    "    [0, 1, 1]\n",
    "])\n",
    "print(active_players.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d446c4-47d5-4536-9947-1875faf43854",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = PokerActionValidator(\n",
    "    num_players = 3,\n",
    "    small_blind = 1,\n",
    "    big_blind = 2,\n",
    "    starting_stack_sizes = 400\n",
    ")\n",
    "\n",
    "legal_actions = test.get_legal_actions_mask(\n",
    "    street_idxs,\n",
    "    table_position_idxs,\n",
    "    action_idxs, \n",
    "    pot_size_sequence,\n",
    "    active_players\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ef66eab-0622-43ab-adb0-33863c767ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True, False,  True, False,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False],\n",
       "        [False, False,  True, False,  True, False,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "          True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False,  True,\n",
       "         False],\n",
       "        [False, False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False],\n",
       "        [False, False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False],\n",
       "        [False, False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False],\n",
       "        [False, False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False],\n",
       "        [False, False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "568f6c50-33b5-4257-8c92-a4507eaf6d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  0, -1, -2,  1,  0,  1,  1,  1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.get_next_to_act(\n",
    "    street_idxs,\n",
    "    table_position_idxs,\n",
    "    action_idxs, \n",
    "    active_players\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5971d3c8-40c4-4a4b-bb7d-dff2f7800dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a025c75b-0275-4f19-9829-07b1dcbd8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "street_embedder = StreetPositionalEncoding(\n",
    "    num_streets = 4,\n",
    "    embedding_dim = 256,\n",
    "    max_seq_len = 1024,\n",
    "    device = \"cuda\"\n",
    ")\n",
    "\n",
    "table_position_embedder = TablePositionalEncoding(\n",
    "    num_players = 3,\n",
    "    embedding_dim = 256,\n",
    "    max_seq_len = 1024,\n",
    "    device = \"cuda\"\n",
    ")\n",
    "\n",
    "action_embedder = ActionEncoding(\n",
    "    #num_actions = 21,\n",
    "    embedding_dim = 256,\n",
    "    max_seq_len = 1024,\n",
    "    device = \"cuda\"\n",
    ")\n",
    "\n",
    "pot_size_embedder = PotSizeSequenceEmbedder(\n",
    "    max_seq_len = 1024,\n",
    "    pad_value = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "93d17c48-7073-47a1-aa73-e16944d904e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_pot_size_sequence = pot_size_embedder(pot_size_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9e6b5c96-5b58-40a0-8e5d-567d746880f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "street_idxs_out, street_embs = street_embedder(street_idxs)\n",
    "street_embedding = {\n",
    "    'street_idxs': street_idxs_out,\n",
    "    'street_embedding': street_embs,\n",
    "}\n",
    "\n",
    "table_pos_idxs_out, table_pos_embs = table_position_embedder(table_position_idxs)\n",
    "table_position_embedding = {\n",
    "    'table_position_idxs': table_pos_idxs_out,\n",
    "    'table_position_embedding': table_pos_embs,\n",
    "}\n",
    "\n",
    "action_idxs_out, action_embs = action_embedder(action_idxs)\n",
    "action_embedding = {\n",
    "    'action_idxs': action_idxs_out,\n",
    "    'action_embedding': action_embs,\n",
    "}\n",
    "\n",
    "model_inputs = street_embedding | table_position_embedding | action_embedding | {'pot_size_sequence' : padded_pot_size_sequence.unsqueeze(2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c520bd0f-b260-4258-ac34-c8f645fc54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_actions = test.get_legal_actions_mask(\n",
    "    street_idxs,\n",
    "    table_position_idxs,\n",
    "    action_idxs, \n",
    "    pot_size_sequence,\n",
    "    active_players\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7c2978b0-ceb5-44f8-9cb2-869317e89051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True, False,  True, False,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False],\n",
       "        [False, False,  True, False,  True, False,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "          True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False,  True,\n",
       "         False],\n",
       "        [False, False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False],\n",
       "        [False, False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False],\n",
       "        [False, False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False],\n",
       "        [False, False, False,  True, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d76f74eb-1649-471e-81a2-3f5aff3ea119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'street_idxs': tensor([[0, 0, 6,  ..., 6, 6, 6],\n",
       "         [0, 0, 0,  ..., 6, 6, 6],\n",
       "         [0, 0, 0,  ..., 6, 6, 6],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 6, 6, 6],\n",
       "         [0, 0, 0,  ..., 6, 6, 6],\n",
       "         [0, 0, 0,  ..., 6, 6, 6]], device='cuda:0'),\n",
       " 'street_embedding': tensor([[[ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          [ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187],\n",
       "          ...,\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187],\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187],\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187]],\n",
       " \n",
       "         [[ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          [ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          [ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          ...,\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187],\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187],\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187]],\n",
       " \n",
       "         [[ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          [ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          [ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          ...,\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187],\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187],\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          [ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          [ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          ...,\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187],\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187],\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187]],\n",
       " \n",
       "         [[ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          [ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          [ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          ...,\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187],\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187],\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187]],\n",
       " \n",
       "         [[ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          [ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          [ 0.1622, -0.5240,  0.1949,  ...,  0.5872,  0.6785, -0.2903],\n",
       "          ...,\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187],\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187],\n",
       "          [-0.6400,  0.9649,  0.5568,  ..., -1.8426,  0.5610,  1.4187]]],\n",
       "        device='cuda:0', grad_fn=<EmbeddingBackward0>),\n",
       " 'table_position_idxs': tensor([[0, 1, 3,  ..., 4, 4, 4],\n",
       "         [0, 1, 2,  ..., 4, 4, 4],\n",
       "         [0, 1, 2,  ..., 4, 4, 4],\n",
       "         ...,\n",
       "         [0, 1, 2,  ..., 4, 4, 4],\n",
       "         [0, 1, 2,  ..., 4, 4, 4],\n",
       "         [0, 1, 2,  ..., 4, 4, 4]], device='cuda:0'),\n",
       " 'table_position_embedding': tensor([[[-0.3513, -0.6151,  1.0135,  ..., -0.9109, -1.4657,  1.0062],\n",
       "          [-1.0860, -0.0454, -0.0995,  ...,  0.5333, -0.8558,  0.5099],\n",
       "          [-1.0617, -0.1553, -1.9208,  ..., -1.3741,  0.4893, -0.6091],\n",
       "          ...,\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925],\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925],\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925]],\n",
       " \n",
       "         [[-0.3513, -0.6151,  1.0135,  ..., -0.9109, -1.4657,  1.0062],\n",
       "          [-1.0860, -0.0454, -0.0995,  ...,  0.5333, -0.8558,  0.5099],\n",
       "          [ 2.4927, -0.2964, -0.8869,  ...,  0.1941, -0.3638,  0.6643],\n",
       "          ...,\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925],\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925],\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925]],\n",
       " \n",
       "         [[-0.3513, -0.6151,  1.0135,  ..., -0.9109, -1.4657,  1.0062],\n",
       "          [-1.0860, -0.0454, -0.0995,  ...,  0.5333, -0.8558,  0.5099],\n",
       "          [ 2.4927, -0.2964, -0.8869,  ...,  0.1941, -0.3638,  0.6643],\n",
       "          ...,\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925],\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925],\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.3513, -0.6151,  1.0135,  ..., -0.9109, -1.4657,  1.0062],\n",
       "          [-1.0860, -0.0454, -0.0995,  ...,  0.5333, -0.8558,  0.5099],\n",
       "          [ 2.4927, -0.2964, -0.8869,  ...,  0.1941, -0.3638,  0.6643],\n",
       "          ...,\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925],\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925],\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925]],\n",
       " \n",
       "         [[-0.3513, -0.6151,  1.0135,  ..., -0.9109, -1.4657,  1.0062],\n",
       "          [-1.0860, -0.0454, -0.0995,  ...,  0.5333, -0.8558,  0.5099],\n",
       "          [ 2.4927, -0.2964, -0.8869,  ...,  0.1941, -0.3638,  0.6643],\n",
       "          ...,\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925],\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925],\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925]],\n",
       " \n",
       "         [[-0.3513, -0.6151,  1.0135,  ..., -0.9109, -1.4657,  1.0062],\n",
       "          [-1.0860, -0.0454, -0.0995,  ...,  0.5333, -0.8558,  0.5099],\n",
       "          [ 2.4927, -0.2964, -0.8869,  ...,  0.1941, -0.3638,  0.6643],\n",
       "          ...,\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925],\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925],\n",
       "          [-1.3442, -0.3804,  0.1799,  ..., -2.1235,  0.0812, -0.3925]]],\n",
       "        device='cuda:0', grad_fn=<EmbeddingBackward0>),\n",
       " 'action_idxs': tensor([[ 0,  1, 21,  ..., 21, 21, 21],\n",
       "         [ 0,  1,  4,  ..., 21, 21, 21],\n",
       "         [ 0,  1,  2,  ..., 21, 21, 21],\n",
       "         ...,\n",
       "         [ 0,  1,  2,  ..., 21, 21, 21],\n",
       "         [ 0,  1,  4,  ..., 21, 21, 21],\n",
       "         [ 0,  1,  5,  ..., 21, 21, 21]], device='cuda:0'),\n",
       " 'action_embedding': tensor([[[-0.9890,  0.2093, -1.2232,  ...,  0.6586,  0.6133, -1.1157],\n",
       "          [-0.0062, -0.3638, -0.9266,  ...,  1.0116, -0.4249,  1.8630],\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115],\n",
       "          ...,\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115],\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115],\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115]],\n",
       " \n",
       "         [[-0.9890,  0.2093, -1.2232,  ...,  0.6586,  0.6133, -1.1157],\n",
       "          [-0.0062, -0.3638, -0.9266,  ...,  1.0116, -0.4249,  1.8630],\n",
       "          [ 0.5680, -0.0502, -0.7488,  ...,  0.2970,  0.6772, -1.0590],\n",
       "          ...,\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115],\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115],\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115]],\n",
       " \n",
       "         [[-0.9890,  0.2093, -1.2232,  ...,  0.6586,  0.6133, -1.1157],\n",
       "          [-0.0062, -0.3638, -0.9266,  ...,  1.0116, -0.4249,  1.8630],\n",
       "          [ 0.7130,  0.7396,  0.0368,  ...,  0.0567, -0.7880,  0.5348],\n",
       "          ...,\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115],\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115],\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.9890,  0.2093, -1.2232,  ...,  0.6586,  0.6133, -1.1157],\n",
       "          [-0.0062, -0.3638, -0.9266,  ...,  1.0116, -0.4249,  1.8630],\n",
       "          [ 0.7130,  0.7396,  0.0368,  ...,  0.0567, -0.7880,  0.5348],\n",
       "          ...,\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115],\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115],\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115]],\n",
       " \n",
       "         [[-0.9890,  0.2093, -1.2232,  ...,  0.6586,  0.6133, -1.1157],\n",
       "          [-0.0062, -0.3638, -0.9266,  ...,  1.0116, -0.4249,  1.8630],\n",
       "          [ 0.5680, -0.0502, -0.7488,  ...,  0.2970,  0.6772, -1.0590],\n",
       "          ...,\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115],\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115],\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115]],\n",
       " \n",
       "         [[-0.9890,  0.2093, -1.2232,  ...,  0.6586,  0.6133, -1.1157],\n",
       "          [-0.0062, -0.3638, -0.9266,  ...,  1.0116, -0.4249,  1.8630],\n",
       "          [-0.9750, -0.8704, -0.6329,  ...,  0.1809,  1.1636, -0.3739],\n",
       "          ...,\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115],\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115],\n",
       "          [-0.0373,  0.5042,  0.3890,  ...,  0.8731, -0.2951, -1.0115]]],\n",
       "        device='cuda:0', grad_fn=<EmbeddingBackward0>),\n",
       " 'pot_size_sequence': tensor([[[ 1.],\n",
       "          [ 3.],\n",
       "          [-1.],\n",
       "          ...,\n",
       "          [-1.],\n",
       "          [-1.],\n",
       "          [-1.]],\n",
       " \n",
       "         [[ 1.],\n",
       "          [ 3.],\n",
       "          [ 5.],\n",
       "          ...,\n",
       "          [-1.],\n",
       "          [-1.],\n",
       "          [-1.]],\n",
       " \n",
       "         [[ 1.],\n",
       "          [ 3.],\n",
       "          [ 3.],\n",
       "          ...,\n",
       "          [-1.],\n",
       "          [-1.],\n",
       "          [-1.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.],\n",
       "          [ 3.],\n",
       "          [ 3.],\n",
       "          ...,\n",
       "          [-1.],\n",
       "          [-1.],\n",
       "          [-1.]],\n",
       " \n",
       "         [[ 1.],\n",
       "          [ 3.],\n",
       "          [ 5.],\n",
       "          ...,\n",
       "          [-1.],\n",
       "          [-1.],\n",
       "          [-1.]],\n",
       " \n",
       "         [[ 1.],\n",
       "          [ 3.],\n",
       "          [ 7.],\n",
       "          ...,\n",
       "          [-1.],\n",
       "          [-1.],\n",
       "          [-1.]]])}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "85501780-a4a1-488c-bdfd-4e574b25ddb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 2048])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ml_modules import *\n",
    "\n",
    "cards = Cards()\n",
    "\n",
    "deck_order = torch.randperm(52)\n",
    "card_embeddings = cards(deck_order%13, deck_order//13)\n",
    "unexposed_card = cards(torch.Tensor([13]).to(dtype = torch.long), torch.Tensor([4]).to(dtype = torch.long))\n",
    "\n",
    "card_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d0492a65-98b4-48d2-b680-8b1927db16de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2048])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unexposed_card.tile([3,1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "eaf8f55a-16e0-4b2b-8d77-126b1e902d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_idxs_all = torch.zeros((2, 52))\n",
    "\n",
    "card_idxs_all[0, :] = deck_order%13\n",
    "card_idxs_all[1, :] = deck_order//13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5f236bd4-ced1-47e0-8b8a-f1dc2434b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_idxs_unexposed = torch.zeros((2, 5))\n",
    "card_idxs_unexposed[0,:] = 13\n",
    "card_idxs_unexposed[1,:] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "90641bf3-c672-4b28-9142-cf432ae99437",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = Player(hole_cards = card_embeddings[:2], card_idxs = card_idxs_all[:, :2], position = 0, active_or_not = 1)\n",
    "bb = Player(hole_cards = card_embeddings[2:4], card_idxs = card_idxs_all[:, 2:4], position = 1, active_or_not = 1)\n",
    "\n",
    "board = Board(board_cards = unexposed_card.tile([5,1]), card_idxs = card_idxs_unexposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c705c3c2-6022-4475-b0e4-977dd2414c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2048])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.board_cards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "739f0ba4-2ce7-4d14-a9cd-0b5d75e12282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonSequenceStateBuilder():\n",
    "    \"\"\"\n",
    "    Grabs necessary elements to build the inputs for the part of the model that are not sequential\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        players : typing.List[torch.nn.Module],\n",
    "        board : torch.nn.Module\n",
    "    ):\n",
    "        self.players = players\n",
    "        self.board = board\n",
    "\n",
    "    def get_state(\n",
    "        self,\n",
    "        position : int\n",
    "    ):\n",
    "        active_players = torch.zeros(len(self.players))\n",
    "        stack_size = torch.zeros(len(self.players))\n",
    "        \n",
    "        for index, player in enumerate(self.players):\n",
    "            if player.position == position:\n",
    "                card_idxs = torch.concat([player.card_idxs, board.card_idxs], dim = 1)\n",
    "                card_embeddings = torch.concat([player.hole_cards, board.board_cards], axis = 0)\n",
    "            active_players[index] = player.active_or_not\n",
    "            stack_size[index] = player.stack_size\n",
    "\n",
    "        return active_players, stack_size, card_idxs, card_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "66e4f358-ce98-42cb-812f-ca721eb09ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sequence_state_builder = NonSequenceStateBuilder(players = [sb, bb], board = board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0d68412a-4f5a-4a60-945a-352d0f794268",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_players, stack_size, card_idxs, card_embeddings = non_sequence_state_builder.get_state(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b90d3fb2-a3b9-4cb0-9e21-daaa2839faa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  4., 13., 13., 13., 13., 13.],\n",
       "        [ 0.,  0.,  4.,  4.,  4.,  4.,  4.]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3b7e9dcd-e414-4d32-8a41-b74881f34f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13., 13., 13., 13., 13.],\n",
       "        [ 4.,  4.,  4.,  4.,  4.]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.card_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "dee80ffa-665f-49e8-86d9-bdf3199ee9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = PolicyModel(\n",
    "    num_players = 2,\n",
    "    active_players_hidden_dims = [1024, 2048],\n",
    "    stack_size_hidden_dims = [1024, 2048],\n",
    "    card_embeddings_hidden_dims = [2048, 2048],\n",
    "    final_output_hidden_dims = [1024, 512, 256]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e7c1b7fd-ea5d-408d-a887-09d1c74cba15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyModel(\n",
       "  (active_players_net): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       "  (stack_size_net): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       "  (card_phi_net): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       "  (card_rho_net): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       "  (final_output_net): Sequential(\n",
       "    (0): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.1, inplace=False)\n",
       "    (9): Linear(in_features=256, out_features=22, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e896a819-d3ff-4ad7-9c28-f021c6af527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "poker_sequence_embedder = PokerSequenceEmbedder(\n",
    "    street_input_dimension = 256,\n",
    "    table_position_input_dimension = 256,\n",
    "    action_input_dimension = 256,\n",
    "    latent_dimensions = [256, 512, 1024, 2048],\n",
    "    device = 'cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "81930ac0-0adc-478a-8c66-265050e7cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs['attention_mask'] = (model_inputs['pot_size_sequence'] != -1).squeeze(-1).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "58881732-f176-48fd-af9f-98587cb1b5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Initial state\n",
      "  Allocated: 30.31 GB\n",
      "  Reserved:  30.91 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  50%|████████████████████████████████████████████████████████████████████████▌                                                                        | 1/2 [00:00<00:00,  5.76it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacity of 31.35 GiB of which 346.62 MiB is free. Including non-PyTorch memory, this process has 30.71 GiB memory in use. Of the allocated memory 30.12 GiB is allocated by PyTorch, and 4.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[178]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m print_gpu_memory(\u001b[33m\"\u001b[39m\u001b[33m1. Initial state\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33m./models/qwen3-1point7b/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m tokenizer, model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m model\n\u001b[32m     22\u001b[39m print_gpu_memory(\u001b[33m\"\u001b[39m\u001b[33m2. After loading model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/local_host_LLM/llm_modules.py:23\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_path)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# load the tokenizer and the model\u001b[39;00m\n\u001b[32m     22\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer, model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/transformers/modeling_utils.py:5048\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5038\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5039\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5041\u001b[39m     (\n\u001b[32m   5042\u001b[39m         model,\n\u001b[32m   5043\u001b[39m         missing_keys,\n\u001b[32m   5044\u001b[39m         unexpected_keys,\n\u001b[32m   5045\u001b[39m         mismatched_keys,\n\u001b[32m   5046\u001b[39m         offload_index,\n\u001b[32m   5047\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5065\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/transformers/modeling_utils.py:5468\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5465\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5467\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5468\u001b[39m         _error_msgs, disk_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5469\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5471\u001b[39m \u001b[38;5;66;03m# Save offloaded index if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/transformers/modeling_utils.py:843\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     disk_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/transformers/modeling_utils.py:770\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[39m\n\u001b[32m    767\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled():\n\u001b[32m    768\u001b[39m         param_device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     _load_parameter_into_model(model, param_name, \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;66;03m# TODO naming is stupid it loads it as well\u001b[39;00m\n\u001b[32m    774\u001b[39m     hf_quantizer.create_quantized_param(model, param, param_name, param_device)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacity of 31.35 GiB of which 346.62 MiB is free. Including non-PyTorch memory, this process has 30.71 GiB memory in use. Of the allocated memory 30.12 GiB is allocated by PyTorch, and 4.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def print_gpu_memory(label=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"{label}\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved:  {reserved:.2f} GB\")\n",
    "        print()\n",
    "\n",
    "# Run through your code with diagnostics\n",
    "print_gpu_memory(\"1. Initial state\")\n",
    "model_name = \"./models/qwen3-1point7b/\"\n",
    "\n",
    "\n",
    "tokenizer, model = load_model(model_name)\n",
    "\n",
    "model\n",
    "\n",
    "print_gpu_memory(\"2. After loading model\")\n",
    "\n",
    "activation = None\n",
    "def hook(_, __, output):\n",
    "    global activation\n",
    "    activation = output\n",
    "\n",
    "handle = model.model.layers[27].post_attention_layernorm.register_forward_hook(hook)\n",
    "print_gpu_memory(\"3. After registering hook\")\n",
    "\n",
    "inputs_embeds = poker_sequence_embedder(model_inputs).to(device=\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "outputs = model(inputs_embeds = inputs_embeds, attention_mask = model_inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "980a5bc5-5796-4c50-a604-96c43adecc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_last_action = activation[torch.arange(activation.shape[0]), model_inputs['attention_mask'].sum(dim = 1) - 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf02734c-0e20-4537-b901-ae75e826164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs['llm_state'] = activations_last_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a90c0e0c-5875-4b44-b483-0a2c1194677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs['active_players'], model_inputs['stack_size'], model_inputs['card_idx'], model_inputs['card_embeddings'] = non_sequence_state_builder.get_state(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b0ab53f-3142-49ce-9734-e1e6a0d7fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs['active_players'] = model_inputs['active_players'].unsqueeze(0).tile([6,1])\n",
    "model_inputs['stack_size'] = model_inputs['stack_size'].unsqueeze(0).tile([6,1])\n",
    "model_inputs['card_idx'] = model_inputs['card_idx'].unsqueeze(0).tile([6,1])\n",
    "model_inputs['card_embeddings'] = model_inputs['card_embeddings'].unsqueeze(0).tile([6,1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a791003-451c-47cd-b471-4a536886f53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 7, 2048])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs['card_embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b184599-e41f-4e0b-b293-db83fef5525a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2215, -1.6380,  0.8224, -0.2931, -0.4615, -0.3304,  0.7759,  1.0198,\n",
       "          1.0651,  0.5802,  1.1974,  2.7783, -0.3674,  0.0615,  0.1738,  0.1880,\n",
       "          0.9507, -0.2243, -1.0851,  0.1643,  1.1359,  0.3762],\n",
       "        [-0.0507,  0.0882,  1.1007,  0.4088, -1.0043, -0.6398, -0.1509,  1.3440,\n",
       "          0.4139,  0.3767, -0.7673,  1.4464, -0.5913,  1.1919, -0.2116,  1.0773,\n",
       "          1.0851, -1.6365, -0.4175,  0.9055, -0.5395, -0.3680],\n",
       "        [-1.1351, -0.1650,  1.1157, -0.5770, -1.0403, -0.1165,  0.8512,  0.7444,\n",
       "         -0.1126, -0.0769, -0.2974,  1.5301, -0.0351,  1.4647,  0.5745,  0.5605,\n",
       "          1.1196, -0.1595, -0.4896, -0.4448,  0.5237,  0.4428],\n",
       "        [ 0.3334, -0.4012,  1.0564, -0.1812, -0.0794,  0.3070,  0.2690,  1.0559,\n",
       "          1.0841, -0.3084,  0.1607,  1.6215, -0.1194,  0.6907,  0.7449,  0.8607,\n",
       "         -0.0980, -0.3606, -0.6934,  0.0763,  1.5088,  0.6603],\n",
       "        [-0.8746,  0.1044,  1.8759, -0.1351, -1.4602, -0.1167,  0.7471,  0.2775,\n",
       "          0.9460,  0.6253, -0.1416,  2.3978,  0.2726,  0.6731,  0.6873,  1.0917,\n",
       "          0.7808,  0.7445, -1.4120, -0.2178,  0.9176,  0.0855],\n",
       "        [-0.4785, -0.7882,  1.1781,  0.2093, -1.2599,  0.8403,  1.1274,  1.4253,\n",
       "          0.1004, -0.3549,  1.3663,  0.4942,  0.4221, -0.3455, -0.3762,  0.6021,\n",
       "          0.5175,  0.5126, -1.6325,  0.2774,  0.4199,  0.5559]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_model(\n",
    "    model_inputs['active_players'].to('cpu'),\n",
    "    model_inputs['stack_size'].to('cpu'),\n",
    "    model_inputs['card_embeddings'].to('cpu'),\n",
    "    model_inputs['llm_state'].to('cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f583efa3-0909-4152-a76f-d7d0e7416383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3173a39b-223f-4b08-a395-f23a53194f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f077e743-533c-4a56-9ef0-8ff7ca40406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokerAgent(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Full poker agent. Contains information about the cards,\n",
    "    the players, the board, the sequence embedding, and the \n",
    "    probability prediction model.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        street_embedder : torch.nn.Module,\n",
    "        table_position_embedder : torch.nn.Module,\n",
    "        action_embedder : torch.nn.Module,\n",
    "        pot_size_embedder : torch.nn.Module,\n",
    "        llm : transformers.AutoModelForCausalLM,\n",
    "        policy_model : torch.nn.Module,\n",
    "        device : str = \"cpu\",\n",
    "        llm_train : bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.llm_train = llm_train\n",
    "\n",
    "        self.street_embedder = street_embedder.to(self.device)\n",
    "        self.table_position_embedder = table_position_embedder.to(self.device)\n",
    "        self.action_embedder = action_embedder.to(self.device)\n",
    "        self.pot_size_embedder = pot_size_embedder.to(self.device)\n",
    "        self.llm = llm\n",
    "        self.policy_model = policy_model.to(self.device)\n",
    "\n",
    "        if not llm_train:\n",
    "            for parameter in self.llm.parameters():\n",
    "                parameter.requires_grad = False\n",
    "\n",
    "        def hook(_, __, output):\n",
    "            global activation\n",
    "            activation = output\n",
    "        \n",
    "        handle = self.llm.model.layers[27].post_attention_layernorm.register_forward_hook(hook)\n",
    "       \n",
    "    def forward(\n",
    "        self,\n",
    "        street_idxs : torch.Tensor,\n",
    "        table_position_idxs : torch.Tensor,\n",
    "        action_idxs : torch.Tensor,\n",
    "        pot_size_sequence : torch.Tensor,\n",
    "        active_players : torch.Tensor,\n",
    "        stack_size : torch.Tensor,\n",
    "        card_embeddings : torch.Tensor\n",
    "    ):\n",
    "        street_idxs_out, street_embs = self.street_embedder(street_idxs)\n",
    "        street_embedding = {\n",
    "            'street_idxs': street_idxs_out,\n",
    "            'street_embedding': street_embs,\n",
    "        }\n",
    "        \n",
    "        table_pos_idxs_out, table_pos_embs = self.table_position_embedder(table_position_idxs)\n",
    "        table_position_embedding = {\n",
    "            'table_position_idxs': table_pos_idxs_out,\n",
    "            'table_position_embedding': table_pos_embs,\n",
    "        }\n",
    "        \n",
    "        action_idxs_out, action_embs = self.action_embedder(action_idxs)\n",
    "        action_embedding = {\n",
    "            'action_idxs': action_idxs_out,\n",
    "            'action_embedding': action_embs,\n",
    "        }\n",
    "        padded_pot_size_sequence = self.pot_size_embedder(pot_size_sequence)\n",
    "        \n",
    "        model_inputs = (\n",
    "            street_embedding \n",
    "            | \n",
    "            table_position_embedding \n",
    "            | \n",
    "            action_embedding \n",
    "            | \n",
    "            {'pot_size_sequence' : padded_pot_size_sequence.unsqueeze(2)}\n",
    "            |\n",
    "            {\n",
    "                'active_players' : active_players,\n",
    "                'stack_size' : stack_size,\n",
    "                'card_embeddings' : card_embeddings\n",
    "            }\n",
    "        )\n",
    "\n",
    "        model_inputs['attention_mask'] = (model_inputs['pot_size_sequence'] != -1).squeeze(-1).to('cuda')\n",
    "\n",
    "        activation = None\n",
    "\n",
    "        inputs_embeds = self.poker_sequence_embedder(model_inputs).to(device=\"cuda\", dtype=torch.bfloat16)\n",
    "        \n",
    "        outputs = self.llm(inputs_embeds = inputs_embeds, attention_mask = model_inputs['attention_mask'])\n",
    "        \n",
    "        activations_last_action = activation[\n",
    "            torch.arange(activation.shape[0]), \n",
    "            model_inputs['attention_mask'].sum(dim = 1) - 1, \n",
    "            :\n",
    "        ]\n",
    "        \n",
    "        model_inputs['llm_state'] = activations_last_action\n",
    "\n",
    "        model_inputs['probits'] = self.policy_model(\n",
    "                model_inputs['active_players'].to(self.device),\n",
    "                model_inputs['stack_size'].to(self.device),\n",
    "                model_inputs['card_embeddings'].to(self.device),\n",
    "                model_inputs['llm_state'].to(self.device)\n",
    "        )\n",
    "\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9e5e80ac-b38d-4b2d-bc02-bbc53c17c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "poker_player = PokerAgent(\n",
    "    street_embedder.to('cuda'),\n",
    "    table_position_embedder.to('cuda'),\n",
    "    action_embedder.to('cuda'),\n",
    "    pot_size_embedder.to('cuda'),\n",
    "    model,\n",
    "    policy_model.to('cuda'),\n",
    "    device = 'cuda',\n",
    "    llm_train = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "758b0bd2-e793-4845-82a2-4b5691a662df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poker_player.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9312c443-bc3f-4d85-88c1-6d13d9e8ea2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyModel(\n",
       "  (active_players_net): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       "  (stack_size_net): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       "  (card_phi_net): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       "  (card_rho_net): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       "  (final_output_net): Sequential(\n",
       "    (0): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.1, inplace=False)\n",
       "    (9): Linear(in_features=256, out_features=22, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poker_player.policy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "449b6b1d-c37f-43a8-832e-40c460e3e1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 3., 3., 3., 3., 3., 3.],\n",
       "        [0., 1., 2., 3., 3., 3., 3., 3.],\n",
       "        [0., 1., 2., 0., 3., 3., 3., 3.],\n",
       "        [0., 1., 2., 0., 1., 3., 3., 3.],\n",
       "        [0., 1., 2., 0., 3., 3., 3., 3.],\n",
       "        [0., 1., 2., 0., 1., 3., 3., 3.],\n",
       "        [0., 1., 2., 0., 1., 3., 3., 3.],\n",
       "        [0., 1., 2., 0., 3., 0., 3., 3.]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_position_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "880fcff9-fa3b-4ad6-80d9-6f0cfb3de26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1., 21., 21., 21., 21., 21., 21.],\n",
       "        [ 0.,  1.,  4., 21., 21., 21., 21., 21.],\n",
       "        [ 0.,  1.,  2.,  2., 21., 21., 21., 21.],\n",
       "        [ 0.,  1.,  2.,  4.,  3., 21., 21., 21.],\n",
       "        [ 0.,  1.,  2.,  4., 21., 21., 21., 21.],\n",
       "        [ 0.,  1.,  2.,  4.,  3., 19., 21., 21.],\n",
       "        [ 0.,  1.,  4.,  2.,  3., 19., 21., 21.],\n",
       "        [ 0.,  1.,  5.,  2.,  4., 19., 17., 21.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9db6fb51-d16a-4d04-99da-f38b0ada6428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  3., -1., -1., -1., -1., -1., -1.],\n",
       "        [ 1.,  3.,  5., -1., -1., -1., -1., -1.],\n",
       "        [ 1.,  3.,  3.,  3., -1., -1., -1., -1.],\n",
       "        [ 1.,  3.,  3.,  4.,  4., -1., -1., -1.],\n",
       "        [ 1.,  3.,  3.,  4., -1., -1., -1., -1.],\n",
       "        [ 1.,  3.,  3.,  4.,  4., -1., -1., -1.],\n",
       "        [ 1.,  3.,  5.,  5.,  5.,  5., -1., -1.],\n",
       "        [ 1.,  3.,  7.,  7.,  9.,  9.,  9., -1.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pot_size_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f7ec5b46-2efe-491c-b88b-7a7565989f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [0., 1., 1.],\n",
       "        [0., 1., 1.]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "593ff78d-9ca0-4c6b-9a17-26faff17c9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_size = torch.Tensor(\n",
    "    [\n",
    "        [399, 398, 400],\n",
    "        [399, 398, 398],\n",
    "        [399, 398, 400],\n",
    "        [398, 398, 400],\n",
    "        [398, 398, 400],\n",
    "        [398, 398, 400],\n",
    "        [399, 398, 398],\n",
    "        [399, 396, 396]\n",
    "    ]\n",
    ")\n",
    "\n",
    "stack_size.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "87745292-037a-4f1b-a35f-9c4d50ee0d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_order_shuffled = torch.argsort(torch.rand(8, 52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "171391bd-106a-4580-8ab6-6837b952d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_order_unshuffled = torch.arange(52).tile(8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "29a53139-81a0-4a12-8459-60a96430ee42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 0, 6, 3, 4, 1, 5])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(deck_order_unshuffled.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "71d3a435-4305-44b5-ace5-d2f31937a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_embeddings = cards(deck_order_shuffled%13, deck_order_shuffled//13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "10494976-6d4b-4181-926a-abf62756c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_unshown_embedding = cards(\n",
    "    torch.Tensor([[13]]).to(dtype = torch.long), torch.Tensor([[4]]).to(dtype = torch.long)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b11b0ab0-b8b4-44c5-9cee-42a32017d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_unshown_embedding = card_unshown_embedding.tile([8, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f05e54d4-cb2c-4deb-83f0-d5e4ca35427c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714]],\n",
       "\n",
       "        [[ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714]],\n",
       "\n",
       "        [[ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714]],\n",
       "\n",
       "        [[ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714]],\n",
       "\n",
       "        [[ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714],\n",
       "         [ 0.1036,  1.7986, -1.5657,  ..., -0.2134,  1.0508, -0.6714]]],\n",
       "       grad_fn=<RepeatBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_unshown_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f8181c05-38ff-4035-b921-2df047aa5fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 2048])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_embeddings[:, :2, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "27105c8a-a94c-4b7f-8b3d-c8b24e8134f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_player_0_embeddings = torch.concat([card_embeddings[:, :2, :], card_unshown_embedding], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "86d028c6-7db0-45a8-8a7a-529811808180",
   "metadata": {},
   "outputs": [],
   "source": [
    "poker_player = poker_player.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c2ba6655-f85f-4617-84ca-932786520c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StreetPositionalEncoding(\n",
       "  (street_embedder): Embedding(7, 256)\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poker_player.street_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f23eec54-8adb-48bc-beef-70e74e6a9803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StreetPositionalEncoding(\n",
       "  (street_embedder): Embedding(7, 256)\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "street_embedder.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "233e0609-5790-4f26-9e46-6c3d37351a4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[153]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpoker_player\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstreet_idxs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtable_position_idxs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43maction_idxs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpot_size_sequence\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactive_players\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstack_size\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcards_player_0_embeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[148]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mPokerAgent.forward\u001b[39m\u001b[34m(self, street_idxs, table_position_idxs, action_idxs, pot_size_sequence, active_players, stack_size, card_embeddings)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     41\u001b[39m     street_idxs : torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m     card_embeddings : torch.Tensor\n\u001b[32m     48\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     street_idxs_out, street_embs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstreet_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstreet_idxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     street_embedding = {\n\u001b[32m     51\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mstreet_idxs\u001b[39m\u001b[33m'\u001b[39m: street_idxs_out,\n\u001b[32m     52\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mstreet_embedding\u001b[39m\u001b[33m'\u001b[39m: street_embs,\n\u001b[32m     53\u001b[39m     }\n\u001b[32m     55\u001b[39m     table_pos_idxs_out, table_pos_embs = \u001b[38;5;28mself\u001b[39m.table_position_embedder(table_position_idxs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/local_host_LLM/sequence_modules.py:47\u001b[39m, in \u001b[36mStreetPositionalEncoding.forward\u001b[39m\u001b[34m(self, street_idxs)\u001b[39m\n\u001b[32m     38\u001b[39m idxs = torch.full(\n\u001b[32m     39\u001b[39m     (B, \u001b[38;5;28mself\u001b[39m.max_seq_len),\n\u001b[32m     40\u001b[39m     fill_value=\u001b[38;5;28mself\u001b[39m.pad_token,\n\u001b[32m     41\u001b[39m     dtype=torch.long,\n\u001b[32m     42\u001b[39m     device=\u001b[38;5;28mself\u001b[39m.device\n\u001b[32m     43\u001b[39m )\n\u001b[32m     45\u001b[39m idxs[:, :L] = street_idxs\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m idxs, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstreet_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43midxs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/torch/nn/modules/sparse.py:192\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/local_llm_host/lib/python3.11/site-packages/torch/nn/functional.py:2542\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2536\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2537\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2539\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2540\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2541\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "poker_player(\n",
    "    street_idxs.to('cuda'),\n",
    "    table_position_idxs.to('cuda'),\n",
    "    action_idxs.to('cuda'), \n",
    "    pot_size_sequence.to('cuda'),\n",
    "    active_players.to('cuda'),\n",
    "    stack_size.to('cuda'),\n",
    "    cards_player_0_embeddings.to('cuda')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753f5b8-8ea2-44ac-80aa-58d2b1504284",
   "metadata": {},
   "outputs": [],
   "source": [
    "street_embedder = StreetPositionalEncoding(\n",
    "    num_streets = 4,\n",
    "    embedding_dim = 256,\n",
    "    max_seq_len = 1024,\n",
    "    device = \"cpu\"\n",
    ")\n",
    "\n",
    "table_position_embedder = TablePositionalEncoding(\n",
    "    num_players = 2,\n",
    "    embedding_dim = 256,\n",
    "    max_seq_len = 1024,\n",
    "    device = \"cpu\"\n",
    ")\n",
    "\n",
    "action_embedder = ActionEncoding(\n",
    "    #num_actions = 21,\n",
    "    embedding_dim = 256,\n",
    "    max_seq_len = 1024,\n",
    "    device = \"cpu\"\n",
    ")\n",
    "\n",
    "pot_size_embedder = PotSizeSequenceEmbedder(\n",
    "    max_seq_len = 1024,\n",
    "    pad_value = -1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
